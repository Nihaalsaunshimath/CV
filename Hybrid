import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim


# Retinex Decomposition Module
class RetinexDecomposition(nn.Module):
    def _init_(self):
        super(RetinexDecomposition, self)._init_()
        self.illumination_net = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        illumination = self.illumination_net(x)
        reflectance = x / (illumination + 1e-6)  # Avoid division by zero
        return illumination, reflectance


# U-Net Enhancement Module
class UNetEnhancement(nn.Module):
    def _init_(self):
        super(UNetEnhancement, self)._init_()

        # Encoder
        self.enc1 = self.contract_block(3, 64)
        self.enc2 = self.contract_block(64, 128)
        self.enc3 = self.contract_block(128, 256)
        self.enc4 = self.contract_block(256, 512)

        # Bottleneck
        self.bottleneck = self.contract_block(512, 1024)

        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dec4 = self.expand_block(1024, 512)
        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = self.expand_block(512, 256)
        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = self.expand_block(256, 128)
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = self.expand_block(128, 64)

        # Final layer
        self.final = nn.Conv2d(64, 3, kernel_size=1, stride=1)

    def contract_block(self, in_channels, out_channels):
        block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        return block

    def expand_block(self, in_channels, out_channels):
        block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU()
        )
        return block

    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(enc1)
        enc3 = self.enc3(enc2)
        enc4 = self.enc4(enc3)

        # Bottleneck
        bottleneck = self.bottleneck(enc4)

        # Decoder
        up4 = self.upconv4(bottleneck)
        dec4 = self.dec4(torch.cat([up4, enc4], dim=1))
        up3 = self.upconv3(dec4)
        dec3 = self.dec3(torch.cat([up3, enc3], dim=1))
        up2 = self.upconv2(dec3)
        dec2 = self.dec2(torch.cat([up2, enc2], dim=1))
        up1 = self.upconv1(dec2)
        dec1 = self.dec1(torch.cat([up1, enc1], dim=1))

        # Final layer
        return self.final(dec1)


# Hybrid Retinex-U-Net Model
class HybridRetinexUNet(nn.Module):
    def _init_(self):
        super(HybridRetinexUNet, self)._init_()
        self.retinex = RetinexDecomposition()
        self.unet = UNetEnhancement()

    def forward(self, x):
        illumination, reflectance = self.retinex(x)
        enhanced_reflectance = self.unet(reflectance)

        # Ensure illumination and enhanced_reflectance have matching spatial dimensions
        if illumination.size() != enhanced_reflectance.size():
            illumination = F.interpolate(illumination, size=enhanced_reflectance.shape[2:], mode="bilinear",
                                         align_corners=False)

        enhanced_image = enhanced_reflectance * illumination
        return enhanced_image


# Dataset Class
class LowLightDataset(Dataset):
    def _init_(self, low_dir, high_dir, transform=None):
        self.low_dir = low_dir
        self.high_dir = high_dir
        self.transform = transform
        self.low_images = sorted(os.listdir(low_dir))
        self.high_images = sorted(os.listdir(high_dir))

    def _len_(self):
        return len(self.low_images)

    def _getitem_(self, idx):
        low_path = os.path.join(self.low_dir, self.low_images[idx])
        high_path = os.path.join(self.high_dir, self.high_images[idx])

        low_image = Image.open(low_path).convert("RGB")
        high_image = Image.open(high_path).convert("RGB")

        if self.transform:
            low_image = self.transform(low_image)
            high_image = self.transform(high_image)

        return low_image, high_image


# Metrics
def calculate_metrics(enhanced, ground_truth):
    enhanced_np = enhanced.detach().cpu().numpy().transpose(0, 2, 3, 1)
    ground_truth_np = ground_truth.detach().cpu().numpy().transpose(0, 2, 3, 1)

    batch_psnr = []
    batch_ssim = []
    batch_mae = []
    batch_mse = []

    for i in range(enhanced_np.shape[0]):
        e = enhanced_np[i]
        g = ground_truth_np[i]

        batch_psnr.append(psnr(g, e, data_range=1.0))
        batch_ssim.append(ssim(g, e, multichannel=True, data_range=1.0))
        batch_mae.append(abs(g - e).mean())
        batch_mse.append(((g - e) ** 2).mean())

    avg_psnr = sum(batch_psnr) / len(batch_psnr)
    avg_ssim = sum(batch_ssim) / len(batch_ssim)
    avg_mae = sum(batch_mae) / len(batch_mae)
    avg_mse = sum(batch_mse) / len(batch_mse)

    return {
        "PSNR": avg_psnr,
        "SSIM": avg_ssim,
        "MAE": avg_mae,
        "MSE": avg_mse
    }


# Training Configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = HybridRetinexUNet().to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Directories
train_low_dir = 'E:\\CV-01\\LOLdataset\\lol_dataset\\our485\\low\\'
train_high_dir = 'E:\\CV-01\\LOLdataset\\lol_dataset\\our485\\high\\'
test_low_dir = 'E:\\CV-01\\LOLdataset\\lol_dataset\\eval15\\high\\'
test_high_dir = 'E:\\CV-01\\LOLdataset\\lol_dataset\\eval15\\high\\'

# Data Loaders
train_dataset = LowLightDataset(train_low_dir, train_high_dir, transform)
test_dataset = LowLightDataset(test_low_dir, test_high_dir, transform)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Training Loop
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for low, high in train_loader:
        low, high = low.to(device), high.to(device)
        optimizer.zero_grad()

        # Forward pass
        output = model(low)

        # Resize the output to match the ground truth size
        output_resized = F.interpolate(output, size=high.shape[2:], mode='bilinear', align_corners=False)

        # Calculate loss
        loss = criterion(output_resized, high)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}")
